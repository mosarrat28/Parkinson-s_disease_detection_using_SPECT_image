{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Reference\n",
        "Help taken from the following sites:\n",
        "https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/\n",
        "https://www.kaggle.com/code/arjunrao2000/beginners-guide-efficientnet-with-keras"
      ],
      "metadata": {
        "id": "0AL4h6qwChwV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLxhguPPtKQ-",
        "outputId": "d04dffe9-6cf1-4e07-a42b-82d6de70da8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqjCQUlYrFSv"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0MfqUQJrtPe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, sampler, random_split\n",
        "from torchvision import models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97Uk6U8EruDl",
        "outputId": "4cf89111-f6a4-4269-d2c9-29f27c724581"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('0.17.1+cu121', '2.2.1+cu121')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "torchvision.__version__, torch.__version__ # ('0.11.2+cu102', '1.10.1+cu102')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wo1yGT1Jrw5P"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sys\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_dwUIrMrz3-"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "import cv2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3E8JtqAbuQYU"
      },
      "outputs": [],
      "source": [
        "def get_classes(data_dir):\n",
        "    all_data = datasets.ImageFolder(data_dir)\n",
        "    return all_data.classes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"/content/drive/MyDrive/Project/dataset-split/train\"\n",
        "test_dir = \"/content/drive/MyDrive/Project/dataset-split/test\"\n",
        "valid_dir = \"/content/drive/MyDrive/Project/dataset-split/val\""
      ],
      "metadata": {
        "id": "qyhLlxdgvTzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcUEQ38gr2aH",
        "outputId": "0dfaa155-3324-416d-c652-ecbc01914314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['non-pd', 'pd'] 2\n"
          ]
        }
      ],
      "source": [
        "dataset_path = \"/content/drive/MyDrive/Project/dataset-split\"\n",
        "classes = get_classes(\"/content/drive/MyDrive/Project/dataset-split/train\")\n",
        "print(classes, len(classes))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-h7_fZh7r9u2"
      },
      "outputs": [],
      "source": [
        "def get_data_loaders(data_dir, batch_size=64, train = False):\n",
        "    if train:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomVerticalFlip(p=0.5),\n",
        "            transforms.RandomApply(torch.nn.ModuleList([transforms.ColorJitter(),\n",
        "                                                        transforms.GaussianBlur(3)]), p=0.1),\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(240),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "            transforms.RandomErasing(p=0.14, value='random')\n",
        "        ])\n",
        "        train_data = datasets.ImageFolder(os.path.join(data_dir, \"train/\"), transform=transform)\n",
        "        print(f\"Found {len(train_data)} images for training with {len(train_data.classes)} classes\")\n",
        "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "        return train_loader, len(train_data)\n",
        "\n",
        "    else:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(240),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "        ])\n",
        "        val_data = datasets.ImageFolder(os.path.join(data_dir, \"val/\"), transform=transform)\n",
        "        test_data = datasets.ImageFolder(os.path.join(data_dir, \"test/\"), transform=transform)\n",
        "        print(f\"Found {len(val_data)} images for validation with {len(val_data.classes)} classes\")\n",
        "        print(f\"Found {len(test_data)} images for testing with {len(test_data.classes)} classes\")\n",
        "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "        return (val_loader, test_loader, len(val_data), len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnpGFtSpsTO4",
        "outputId": "a70e80ed-f52f-496f-bbfe-c14084c9bc6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1369 images for training with 2 classes\n",
            "Found 391 images for validation with 2 classes\n",
            "Found 196 images for testing with 2 classes\n"
          ]
        }
      ],
      "source": [
        "(train_loader, train_data_len) = get_data_loaders(dataset_path, 64, train=True)\n",
        "(val_loader, test_loader, valid_data_len, test_data_len) = get_data_loaders(dataset_path, 16, train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBsyaLOIsWHy"
      },
      "outputs": [],
      "source": [
        "dataloaders = {\n",
        "    \"train\":train_loader,\n",
        "    \"val\": val_loader\n",
        "}\n",
        "dataset_sizes = {\n",
        "    \"train\":train_data_len,\n",
        "    \"val\": valid_data_len\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZJEJz-XsW3u",
        "outputId": "5992b20c-9e64-4923-ae24-28e146661673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n",
            "25\n",
            "13\n"
          ]
        }
      ],
      "source": [
        "print(len(train_loader))\n",
        "print(len(val_loader))\n",
        "print(len(test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIdTzHnvsZro",
        "outputId": "b548136e-ce81-48e7-f081-66541418c27d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1369 196 391\n"
          ]
        }
      ],
      "source": [
        "print(train_data_len, test_data_len, valid_data_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyNUufmCsefp",
        "outputId": "18e566f2-1208-43e9-8e5b-268bbb505ea5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey7TElcVsnU7",
        "outputId": "fa5ccfb3-3b34-4f69-a791-6aa2d32d5cec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B7_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B7_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b7_lukemelas-c5b4e57e.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b7_lukemelas-c5b4e57e.pth\n",
            "100%|██████████| 255M/255M [00:14<00:00, 18.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=2560, out_features=2048, bias=True)\n",
            "  (1): SiLU()\n",
            "  (2): Dropout(p=0.2, inplace=False)\n",
            "  (3): Linear(in_features=2048, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "torch.backends.cudnn.benchmark = True\n",
        "model = models.efficientnet_b7(pretrained=True)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "n_inputs = model.classifier[1].in_features\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(n_inputs,2048),\n",
        "    nn.SiLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(2048, len(classes))\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "print(model.classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIju7byMstdY"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.11)\n",
        "criterion = criterion.to(device)\n",
        "optimizer = optim.AdamW(model.classifier.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGA5s72asuEb"
      },
      "outputs": [],
      "source": [
        "training_history = {'accuracy':[],'loss':[]}\n",
        "validation_history = {'accuracy':[],'loss':[]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrc6vrTEswpb"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import time\n",
        "import copy\n",
        "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOud3hiasz0j"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in tqdm(dataloaders[phase]):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            if phase == 'train':\n",
        "                training_history['accuracy'].append(epoch_acc)\n",
        "                training_history['loss'].append(epoch_loss)\n",
        "            elif phase == 'val':\n",
        "                validation_history['accuracy'].append(epoch_acc)\n",
        "                validation_history['loss'].append(epoch_loss)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_hist(hist):\n",
        "    plt.plot(hist.training_history[\"accuracy\"])\n",
        "    #plt.plot(hist.history[\"val_accuracy\"])\n",
        "    plt.title(\"model accuracy\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2Iq6LtPcGx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-K2iqQMs-La",
        "outputId": "57a04648-e224-4935-a9eb-b33403f96b61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [08:52<00:00, 24.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3943 Acc: 0.8897\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [02:11<00:00,  5.27s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 1.5453 Acc: 0.5090\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [08:23<00:00, 22.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3450 Acc: 0.9248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:30<00:00,  3.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 1.1651 Acc: 0.4987\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [08:34<00:00, 23.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3286 Acc: 0.9350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:38<00:00,  3.95s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.9452 Acc: 0.5345\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [08:23<00:00, 22.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3165 Acc: 0.9503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:35<00:00,  3.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.6049 Acc: 0.7212\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [08:14<00:00, 22.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3180 Acc: 0.9401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:25<00:00,  3.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.5265 Acc: 0.7877\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [08:35<00:00, 23.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3147 Acc: 0.9445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:26<00:00,  3.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.4861 Acc: 0.8159\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [08:01<00:00, 21.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3227 Acc: 0.9438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:26<00:00,  3.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.4593 Acc: 0.8414\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [07:58<00:00, 21.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3179 Acc: 0.9489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:38<00:00,  3.94s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.4346 Acc: 0.8491\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [08:05<00:00, 22.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3227 Acc: 0.9364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:24<00:00,  3.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.3939 Acc: 0.8824\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [08:23<00:00, 22.89s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3200 Acc: 0.9401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:32<00:00,  3.69s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.3692 Acc: 0.9054\n",
            "\n",
            "Training complete in 99m 26s\n",
            "Best val Acc: 0.905371\n"
          ]
        }
      ],
      "source": [
        "model = train_model(model, criterion, optimizer, exp_lr_scheduler,\n",
        "                       num_epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmgYzQCttAmG",
        "outputId": "3c972cc7-9ac4-4115-9dac-034dbf9b47ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [01:18<00:00,  6.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.358478\n",
            "\n",
            "Test Accuracy of non-pd: 90% (85/94)\n",
            "Test Accuracy of    pd: 90% (89/98)\n",
            "\n",
            "Test Accuracy (Overall): 90.6250 (174.0/192.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def test(model):\n",
        "  test_loss = 0.0\n",
        "  class_correct = list(0. for i in range(len(classes)))\n",
        "  class_total = list(0. for i in range(len(classes)))\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  for data, target in tqdm(test_loader):\n",
        "      if torch.cuda.is_available():\n",
        "          data, target = data.cuda(), target.cuda()\n",
        "      with torch.no_grad():\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "      test_loss += loss.item()*data.size(0)\n",
        "      _, pred = torch.max(output, 1)\n",
        "      correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "      correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor.cpu().numpy())\n",
        "      if len(target) == 16:\n",
        "        for i in range(16):\n",
        "            label = target.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "  test_loss = test_loss/len(test_loader.dataset)\n",
        "  print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "  for i in range(len(classes)):\n",
        "      if class_total[i] > 0:\n",
        "          print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "              classes[i], 100 * class_correct[i] / class_total[i],\n",
        "              np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "      else:\n",
        "          print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "  print('\\nTest Accuracy (Overall): {:.4f} ({}/{})'.format(\n",
        "      100. * np.sum(class_correct) / np.sum(class_total),\n",
        "      np.sum(class_correct), np.sum(class_total)))\n",
        "test(model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}